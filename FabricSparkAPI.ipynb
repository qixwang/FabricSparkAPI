{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time \n",
    "\n",
    "ws_id = \"13d64313-5498-42f3-bf8b-9b2b83e1a4d4\"\n",
    "lakehouse_id= \"3d0191f6-e627-47c7-a289-5b201bc45ca9\"\n",
    "\n",
    "api_base_url_mist='https://msitapi.fabric.microsoft.com/v1/'\n",
    "api_base_url_daily ='https://dailyapi.fabric.microsoft.com/v1/'\n",
    "api_base_url_dxt = 'https://dxtapi.fabric.microsoft.com/v1/'\n",
    "\n",
    "api_url = api_base_url_daily\n",
    "\n",
    "with open('token.txt', 'r') as file:\n",
    "    access_token = file.read().strip()\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer \" + access_token}\n",
    "\n",
    "def printResponse(response):\n",
    "   print(f\"API call return with status code: {response.status_code}\")\n",
    "   print(response.json())\n",
    "   print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the list workspace API to check if the token is valid\n",
    "\n",
    "get_ws_url= api_url+\"workspaces/ff84063b-d6c5-4684-b427-c17c881b6110\"\n",
    "get_ws_response = requests.get(get_ws_url, headers=headers)\n",
    "print(get_ws_response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new ws and assign the capacity \n",
    "# create_ws_url= api_url+\"workspaces\"\n",
    "\n",
    "# payload_data = {\n",
    "#     \"displayName\": \"WS4SparkAPIDemo\"\n",
    "# }\n",
    "# create_ws_response = requests.post(create_ws_url, headers=headers,json=payload_data)\n",
    "# print(create_ws_response.json())\n",
    "# new_ws_id = create_ws_response.json()['id']\n",
    "\n",
    "new_ws_id = 'ff84063b-d6c5-4684-b427-c17c881b6110'\n",
    "\n",
    "# assignCapacity_url = api_url +\"workspaces/\" +new_ws_id+ \"assignToCapacity\"\n",
    "# assignCapacity_payload ={\n",
    "#     \"capacityId\": \"0f084df7-c13d-451b-af5f-ed0c466403b2\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the ws settings related with Spark\n",
    "get_spark_ws_settings =  api_url+\"workspaces/\"+new_ws_id+\"/spark/settings/\"\n",
    "get_spark_settings_response = requests.get(get_spark_ws_settings, headers=headers)\n",
    "printResponse(get_spark_settings_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create custom pool \n",
    "create_pool_url = api_url+\"workspaces/\"+new_ws_id+\"/spark/pools\"\n",
    "wild_cat = 'wildcat2'\n",
    "create_pool_payload = {\n",
    "    \"name\": wild_cat,\n",
    "    \"nodeFamily\": \"MemoryOptimized\",\n",
    "    \"nodeSize\": \"Small\",\n",
    "    \"autoScale\": {\n",
    "        \"enabled\": 'true',\n",
    "        \"minNodeCount\": 1,\n",
    "        \"maxNodeCount\": 2\n",
    "    },\n",
    "    \"dynamicExecutorAllocation\": {\n",
    "        \"enabled\": 'true',\n",
    "        \"minExecutors\": 1,\n",
    "        \"maxExecutors\": 1\n",
    "    }\n",
    "}\n",
    "create_pool_response = requests.post(create_pool_url, headers=headers,json=create_pool_payload)\n",
    "printResponse(create_pool_response)\n",
    "new_pool_id = create_pool_response.json()['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to get all the pools \n",
    "get_pool_url = api_url+\"workspaces/\"+new_ws_id+\"/spark/pools\"\n",
    "get_pool_url = requests.get(get_pool_url, headers=headers)\n",
    "printResponse(get_pool_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the ws spark settings by setting the default pool to the new pool and set the default runtime version\n",
    "\n",
    "starter_pool = \"Starter Pool\"\n",
    "another_pool = \"wildcat\"\n",
    "update_spark_setting_url = api_url+\"workspaces/\"+new_ws_id+\"/spark/settings/\"\n",
    "update_spark_setting_payload={\n",
    "    \"pool\": {\n",
    "        \"customizeComputeEnabled\": 'true',\n",
    "        \"defaultPool\": {\n",
    "            \"name\": starter_pool,\n",
    "            \"type\": \"Workspace\"\n",
    "        }\n",
    "    },\n",
    "    \"environment\": {\n",
    "        \"runtimeVersion\": \"1.2\"\n",
    "    }\n",
    "}\n",
    "update_spark_setting_response = requests.patch(update_spark_setting_url, headers=headers, json=update_spark_setting_payload)\n",
    "printResponse(update_spark_setting_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let create a new environment artifact and set it as the default env of the workspace\n",
    "\n",
    "newenv = \"starterEnv3\"\n",
    "\n",
    "create_env_url = api_url + \"/workspaces/\"+new_ws_id+\"/items\"\n",
    "create_env_payload = {\n",
    "    \"displayName\": newenv,\n",
    "    \"type\": 'Environment'\n",
    "}\n",
    "create_env_response = requests.post(create_env_url, headers=headers, json=create_env_payload)\n",
    "printResponse(create_env_response)\n",
    "\n",
    "# update this env with certain settings\n",
    "\n",
    "env_id = create_env_response.json()['id']\n",
    "#env_id = '3c52ddf8-df2e-45c0-9fdb-55f7b9fbcbb7'\n",
    "update_env_url = api_url + \"/workspaces/\"+new_ws_id+\"/environments/\"+env_id+\"/staging/sparkcompute\"\n",
    "update_env_payload = {\n",
    "    \"instancePool\": {\n",
    "    \"name\": \"Starter Pool\",\n",
    "    \"type\": \"Workspace\"\n",
    "  },\n",
    "  \"driverCores\": 4,\n",
    "  \"driverMemory\": \"28g\",\n",
    "  \"executorCores\": 4,\n",
    "  \"executorMemory\": \"28g\",\n",
    "  \"dynamicExecutorAllocation\": {\n",
    "    \"enabled\": 'true',\n",
    "    \"minExecutors\": 1,\n",
    "    \"maxExecutors\": 2\n",
    "  },\n",
    "  \"sparkProperties\": {\n",
    "    \"spark.setting1\": \"setting2222\",\n",
    "    \"spark.setting2\": \"settings3\"\n",
    "  },\n",
    "  \"runtimeVersion\": \"1.1\"\n",
    "}\n",
    "update_env_response = requests.patch(update_env_url, headers=headers, json=update_env_payload)\n",
    "printResponse(update_env_response)\n",
    "\n",
    "# set this env as the default env of the workspace\n",
    "\n",
    "set_default_env_url = api_url+\"workspaces/\"+new_ws_id+\"/spark/settings/\"\n",
    "set_default_env_payload={\n",
    "    \"environment\": {\n",
    "        \"name\": \"starterEnv3\"\n",
    "    }\n",
    "}\n",
    "set_default_env_response = requests.patch(set_default_env_url, headers=headers, json=set_default_env_payload)\n",
    "printResponse(set_default_env_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Lakehouse with the name as LHAPIDemo within this workspace\n",
    "\n",
    "newlakehouseName = \"LHAPIDemo\"\n",
    "\n",
    "create_lakehouse = api_url + \"/workspaces/\"+new_ws_id+\"/items\"\n",
    "create_lakehouse_payload = {\n",
    "    \"displayName\": newlakehouseName,\n",
    "    \"type\": 'Lakehouse'\n",
    "}\n",
    "create_lakehouse_response = requests.post(create_lakehouse, headers=headers, json=create_lakehouse_payload)\n",
    "printResponse(create_lakehouse_response)\n",
    "lakehouse_id = create_lakehouse_response.json()['id']\n",
    "\n",
    "# Assume under the File section of this lakehouse, we have a file named \"yellow_tripdata_2022_01.csv\"\n",
    "# Now we can call the Load to Table API to create a delta table from this csv file, a new table named \"yellowtrip\" will be created under the Lakehouse\n",
    "\n",
    "#lakehouse_id ='0a48b3d9-b277-4555-a0cd-f87804a4e821'\n",
    "tablename = \"yellowtrip\"\n",
    "load_to_table_url = api_url + \"/workspaces/\"+new_ws_id+\"/lakehouses/\"+lakehouse_id+\"/tables/\"+tablename+\"/load \"\n",
    "load_to_table_payload = {\n",
    "    \"relativePath\": \"Files/yellow_tripdata_2022_01.csv\",\n",
    "    \"pathType\": \"File\",\n",
    "    \"mode\": \"overwrite\",\n",
    "    \"formatOptions\": {\n",
    "        \"header\": 'true', \n",
    "        \"delimiter\": \",\", \n",
    "        \"format\": \"CSV\" \n",
    "    }\n",
    "}\n",
    "\n",
    "load_to_table_response = requests.post(load_to_table_url, headers=headers, json=load_to_table_payload)\n",
    "\n",
    "print(load_to_table_response.headers['Location'])\n",
    "operation_id = load_to_table_response.headers['Location'].split('/')[-1]\n",
    "print(operation_id)\n",
    "\n",
    "# Now we can call the get operation to check the status of the load operation\n",
    "\n",
    "# get_load_status_url = api_url + \"/workspaces/\"+new_ws_id+\"/lakehouses/\"+lakehouse_id+\"/operations/\"+operation_id\n",
    "# get_load_status_response = requests.get(get_load_status_url, headers=headers)\n",
    "# print(get_load_status_response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a new notebook with one code cell as : \n",
    "# df = spark.sql(\"SELECT * FROM LHAPIDemo.`yellow_tripdata` LIMIT 1000\")\n",
    "# display(df) \n",
    "\n",
    "new_notebook_name = \"NotebookAPIDemo\"\n",
    "new_notebook_content_in_base64 = \"eyJjZWxscyI6W3siY2VsbF90eXBlIjoiY29kZSIsInNvdXJjZSI6WyJkZiA9IHNwYXJrLnNxbChcIlNFTEVDVCAqIEZST00gTEhBUElEZW1vLmB5ZWxsb3dfdHJpcGRhdGFgIExJTUlUIDEwMDBcIilcbiIsImRpc3BsYXkoZGYpIl0sIm91dHB1dHMiOltdLCJleGVjdXRpb25fY291bnQiOm51bGwsIm1ldGFkYXRhIjp7fSwiaWQiOiI2YWM3YTAwZC1iMjhiLTRjZjctYmQ4OC05ZWMzY2E5NjhkYzcifV0sIm1ldGFkYXRhIjp7Imxhbmd1YWdlX2luZm8iOnsibmFtZSI6InB5dGhvbiJ9LCJrZXJuZWxfaW5mbyI6eyJuYW1lIjoic3luYXBzZV9weXNwYXJrIn0sIm1pY3Jvc29mdCI6eyJsYW5ndWFnZSI6InB5dGhvbiJ9LCJ3aWRnZXRzIjp7fSwibnRlcmFjdCI6eyJ2ZXJzaW9uIjoibnRlcmFjdC1mcm9udC1lbmRAMS4wLjAifSwic3BhcmtfY29tcHV0ZSI6eyJjb21wdXRlX2lkIjoiL3RyaWRlbnQvZGVmYXVsdCJ9LCJkZXBlbmRlbmNpZXMiOnsibGFrZWhvdXNlIjp7ImRlZmF1bHRfbGFrZWhvdXNlIjoiMGE0OGIzZDktYjI3Ny00NTU1LWEwY2QtZjg3ODA0YTRlODIxIiwiZGVmYXVsdF9sYWtlaG91c2VfbmFtZSI6IkxIQVBJRGVtbyIsImRlZmF1bHRfbGFrZWhvdXNlX3dvcmtzcGFjZV9pZCI6ImZmODQwNjNiLWQ2YzUtNDY4NC1iNDI3LWMxN2M4ODFiNjExMCJ9fX0sIm5iZm9ybWF0Ijo0LCJuYmZvcm1hdF9taW5vciI6NX0=\"\n",
    "\n",
    "create_notebook_url = api_url + \"/workspaces/\"+new_ws_id+\"/items\"\n",
    "create_notebook_payload = {\n",
    "    \"displayName\": new_notebook_name,\n",
    "    \"type\": 'Notebook',\n",
    "    \"definition\" : {\n",
    "        \"format\": \"ipynb\",\n",
    "        \"parts\": [\n",
    "            {\n",
    "                \"path\": \"notebook-content.ipynb\",\n",
    "                \"payload\": new_notebook_content_in_base64,\n",
    "                \"payloadType\": \"InlineBase64\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "create_notebook_response = requests.post(create_notebook_url, headers=headers, json=create_notebook_payload)\n",
    "printResponse(create_notebook_response)\n",
    "\n",
    "#Now let's trigger the execution of this notebook \n",
    "\n",
    "notebook_id = create_notebook_response.json()['id']\n",
    "#notebook_id = '1326f424-ccb5-4a53-9734-8f4c15bcb748'    \n",
    "trigger_notebook_url = api_url + \"/workspaces/\"+new_ws_id+\"/items/\"+notebook_id+\"/jobs/instances?jobType=RunNotebook\"\n",
    "trigger_notebook_payload = {\n",
    "    \"executionData\": {\n",
    "        \"parameters\": {\n",
    "            \"para1\": {\n",
    "                \"value\": \"new path\",\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "trigger_notebook_response = requests.post(trigger_notebook_url, headers=headers, json=trigger_notebook_payload)\n",
    "print(trigger_notebook_response.headers['Location'])\n",
    "job_id = trigger_notebook_response.headers['Location'].split('/')[-1]\n",
    "print(job_id)\n",
    "\n",
    "# Now let's check the status of this job\n",
    "# job_id = '2c02a9fd-4df7-4b8a-8c0a-47f94c78c6e3'\n",
    "# notebook_id = '1326f424-ccb5-4a53-9734-8f4c15bcb748'\n",
    "get_job_status_url = api_url + \"/workspaces/\"+new_ws_id+\"/items/\"+notebook_id+\"/jobs/instances/\"+job_id\n",
    "get_job_status_response = requests.get(get_job_status_url, headers=headers)\n",
    "print(get_job_status_response.json())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a new Livy session and execute a statement to query the data in the delta table\n",
    "\n",
    "lakehouse_id ='f715aa5d-493f-401b-8f8c-62d9bcd19f57'\n",
    "ws_id = '833c95bb-47e2-4592-b5ff-39ccb854cc39'\n",
    "livy_base_url = api_base_url_mist + \"/workspaces/\"+ws_id+\"/lakehouses/\"+lakehouse_id +\"/livyApi/versions/2023-12-01/\"\n",
    "\n",
    "create_livy_session = livy_base_url + \"sessions\"\n",
    "payload_data = {}\n",
    "create_session = requests.post(create_livy_session, headers=headers,json=payload_data)\n",
    "session_id = create_session.json()['id']\n",
    "\n",
    "print('The request to create Livy session is submitted:' + str(create_session.json()))\n",
    "\n",
    "get_livy_session =  livy_base_url+\"sessions/\" + session_id\n",
    "get_session_response = requests.get(get_livy_session, headers=headers)\n",
    "#print(get_session_response.json())\n",
    "\n",
    "while get_session_response.json()[\"state\"] != \"idle\":\n",
    "    # Sleep for 5 seconds before making the next request\n",
    "    time.sleep(5)\n",
    "    #print('the statement code is submitted and running  : ' + str(execute_statement_response.json()))\n",
    "\n",
    "    # Make the next request\n",
    "    get_session_response = requests.get(get_livy_session, headers=headers)\n",
    "\n",
    "\n",
    "execute_statement = livy_base_url + \"sessions/\" + session_id +\"/statements\"\n",
    "payload_data =    {\n",
    "      \"code\": \"spark.sql(\\\"SELECT * FROM ContosoOutdoors.Transactions where TotalPrice < 0\\\").show()\",\n",
    "      \"kind\": \"spark\"\n",
    "    }\n",
    "execute_statement_response = requests.post(execute_statement, headers=headers,json=payload_data)\n",
    "#print('/////////////////////////')\n",
    "#print(get_session_response.json())\n",
    "print('the statement code is submitted as: ' + str(execute_statement_response.json()))\n",
    "\n",
    "statement_id = str(execute_statement_response.json()['id'])\n",
    "get_statement = livy_base_url +\"sessions/\"+ session_id + \"/statements/\" + statement_id\n",
    "get_statement_response = requests.get(get_statement, headers=headers)\n",
    "\n",
    "while get_statement_response.json()[\"state\"] != \"available\":\n",
    "    # Sleep for 5 seconds before making the next request\n",
    "    time.sleep(5)\n",
    "    print('the statement code is submitted and running  : ' + str(execute_statement_response.json()))\n",
    "\n",
    "    # Make the next request\n",
    "    get_statement_response = requests.get(get_statement, headers=headers)\n",
    "\n",
    "\n",
    "rst = get_statement_response.json()\n",
    " \n",
    "# Assuming result is your returned data\n",
    "result_data = rst['output']['data']['text/plain']\n",
    "print(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"items\":[{\"id\":\"6a8b0395-06df-43d4-bbb1-64dcd269025c\",\"name\":\"livybatchdemo_withLHAPIDemo2\",\"workspaceId\":\"833c95bb-47e2-4592-b5ff-39ccb854cc39\",\"submitterId\":\"a7d2bd4a-be4e-4d7d-8787-a07cdcb4e95b\",\"submitterName\":\"qixwang@microsoft.com\",\"artifactId\":\"f715aa5d-493f-401b-8f8c-62d9bcd19f57\",\"result\":\"Uncertain\",\"submittedAt\":\"2024-03-21T11:30:51.0293977+00:00\",\"startedAt\":\"2024-03-21T11:30:57.209397+00:00\",\"tags\":{\"ArtifactJobType\":\"LivyBatch\",\"JobInvokeType\":\"Livy API\",\"maxAllowedCoresForJob\":\"80\",\"artifactName\":\"FabricLivyDemo\",\"operationName\":\"Batch Livy Run\",\"appliedThrottling\":\"{\\\"ThrottlingType\\\":0,\\\"Value\\\":0.0}\",\"isBillable\":\"True\",\"tenantId\":\"72f988bf-86f1-41af-91ab-2d7cd011db47\",\"capacityId\":\"483c2de8-fb78-4b2a-bc72-3218d2764314\",\"workspaceId\":\"833c95bb-47e2-4592-b5ff-39ccb854cc39\",\"artifactId\":\"f715aa5d-493f-401b-8f8c-62d9bcd19f57\",\"artifactKind\":\"Lakehouse\",\"consumerIdentity\":\"\",\"fabricRuntimeVersion\":\"1.2\"},\"schedulerState\":\"Scheduled\",\"pluginState\":\"Monitoring\",\"livyState\":\"starting\",\"isJobTimedOut\":false},{\"id\":\"27ab21f2-fddd-4301-85d6-f0d15f97f850\",\"name\":\"livybatchdemo_withLHAPIDemo2\",\"workspaceId\":\"833c95bb-47e2-4592-b5ff-39ccb854cc39\",\"submitterId\":\"a7d2bd4a-be4e-4d7d-8787-a07cdcb4e95b\",\"submitterName\":\"qixwang@microsoft.com\",\"artifactId\":\"f715aa5d-493f-401b-8f8c-62d9bcd19f57\",\"cancellationReason\":\"Cancelled by user.\",\"result\":\"Cancelled\",\"submittedAt\":\"2024-03-21T11:29:18.3971826+00:00\",\"startedAt\":\"2024-03-21T11:29:24.5565574+00:00\",\"endedAt\":\"2024-03-21T11:30:35.5505044+00:00\",\"tags\":{\"ArtifactJobType\":\"LivyBatch\",\"JobInvokeType\":\"Livy API\",\"maxAllowedCoresForJob\":\"80\",\"artifactName\":\"FabricLivyDemo\",\"operationName\":\"Batch Livy Run\",\"appliedThrottling\":\"{\\\"ThrottlingType\\\":0,\\\"Value\\\":0.0}\",\"isBillable\":\"True\",\"tenantId\":\"72f988bf-86f1-41af-91ab-2d7cd011db47\",\"capacityId\":\"483c2de8-fb78-4b2a-bc72-3218d2764314\",\"workspaceId\":\"833c95bb-47e2-4592-b5ff-39ccb854cc39\",\"artifactId\":\"f715aa5d-493f-401b-8f8c-62d9bcd19f57\",\"artifactKind\":\"Lakehouse\",\"consumerIdentity\":\"\",\"fabricRuntimeVersion\":\"1.2\"},\"schedulerState\":\"Ended\",\"pluginState\":\"Ended\",\"livyState\":\"killed\",\"isJobTimedOut\":false}],\"totalCountOfMatchedItems\":2,\"pageSize\":100}\n"
     ]
    }
   ],
   "source": [
    "# call get batch API\n",
    "lakehouse_id ='f715aa5d-493f-401b-8f8c-62d9bcd19f57'\n",
    "ws_id = '833c95bb-47e2-4592-b5ff-39ccb854cc39'\n",
    "livy_base_url = api_base_url_mist + \"/workspaces/\"+ws_id+\"/lakehouses/\"+lakehouse_id +\"/livyApi/versions/2023-12-01/\"\n",
    "get_livy_get_batch = livy_base_url+\"batches\"\n",
    "get_batch_response = requests.get(get_livy_get_batch, headers=headers)\n",
    "print(get_batch_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API call return with status code: 201\n",
      "{'id': '6eadf6aa-97eb-4153-9478-2e07c0ca0f6b', 'type': 'Lakehouse', 'displayName': 'LHAPIDemo2', 'description': '', 'workspaceId': '833c95bb-47e2-4592-b5ff-39ccb854cc39'}\n",
      "{\"id\":\"6eadf6aa-97eb-4153-9478-2e07c0ca0f6b\",\"type\":\"Lakehouse\",\"displayName\":\"LHAPIDemo2\",\"description\":\"\",\"workspaceId\":\"833c95bb-47e2-4592-b5ff-39ccb854cc39\"}\n",
      "The Livy batch job submitted successful\n",
      "{'id': '6a8b0395-06df-43d4-bbb1-64dcd269025c', 'artifactId': 'f715aa5d-493f-401b-8f8c-62d9bcd19f57'}\n"
     ]
    }
   ],
   "source": [
    "# Now let's submit a spark job via the livy batch API, this job will run a .py script set by the path in the payload.\n",
    "# The spark job will write the result to delta table in the lakehouse created as LHAPIDemo2\n",
    " \n",
    "lakehouse_id ='f715aa5d-493f-401b-8f8c-62d9bcd19f57'\n",
    "ws_id = '833c95bb-47e2-4592-b5ff-39ccb854cc39'\n",
    "livy_base_url = api_base_url_mist + \"/workspaces/\"+ws_id+\"/lakehouses/\"+lakehouse_id +\"/livyApi/versions/2023-12-01/\"\n",
    "\n",
    "\n",
    "newlakehouseName = \"LHAPIDemo2\"\n",
    "\n",
    "create_lakehouse = api_base_url_mist + \"/workspaces/\"+ws_id+\"/items\"\n",
    "create_lakehouse_payload = {\n",
    "    \"displayName\": newlakehouseName,\n",
    "    \"type\": 'Lakehouse'\n",
    "}\n",
    "create_lakehouse_response = requests.post(create_lakehouse, headers=headers, json=create_lakehouse_payload)\n",
    "printResponse(create_lakehouse_response)\n",
    "\n",
    "\n",
    "run_batch_job = livy_base_url+\"batches\"\n",
    "payload_data = {\n",
    "    \"name\":\"livybatchdemo_with\"+ newlakehouseName,\n",
    "    \"file\":\"abfss://ContosoOutdoor@msit-onelake.dfs.fabric.microsoft.com/ContosoOutdoors.Lakehouse/Files/livybatch/livybatchdemo.py\",\n",
    "    \"conf\": {\n",
    "        \"spark.targetLakehouse\": newlakehouseName\n",
    "\n",
    "    }\n",
    "}\n",
    "get_batch_response = requests.post(run_batch_job, headers=headers,json=payload_data)\n",
    "\n",
    "print(\"The Livy batch job submitted successful\")\n",
    "print(get_batch_response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
